1. Data Ingestion
    *** Using synthetic emails for project ***
    Sources: Outlook/Sharepoint
    Method: Scheduled ETL Scripts
    Storage: PostgreSQL (Supabase)

2. Data Preprocessing (NLP)
    Steps:
        - Parse Email Text
        - Extract Entities: teams, persons, case IDs, dates
        - Relation Extraction: Who communicates with whom, which case
    Tools: spaCy
    Output: structured table (email_entities)

3. Knowledge Graph
    Nodes: People, Teams, Cases
    Edges: Communication links, case involvement
    Library: NetworkX + PyVis for visualization
    Storage: Serialize graph (GraphML/JSON)

4. Summarization
    Generate per-case updates:
        - Transformer based
    Store Summaries in case_summaries table

5. API/Frontend
    Framework: FastAPI
    Endpoints: 
        /cases -> list all cases
        /cases/{id}/summary -> case updates
        /graph -> interactive knowledge graph
    OAuth?

6. Scheduling & Automation
    Cron or Airflow?
        - Fetch new emails 
        - Update NLP & graph
        - Regenerate Summaries
    
7. Logging & Monitoring
    Logging: Python logging module

8. Deployment
    Vercel & FastAPI